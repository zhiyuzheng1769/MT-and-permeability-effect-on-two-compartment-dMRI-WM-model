{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b641efba-c1e6-4c5a-a7fa-0b185a5da013",
   "metadata": {},
   "source": [
    "# MT-related figures\n",
    "This notebook reproduces result figures in the paper that came from the fixed diameter cases, with MT effects only (Figure 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a6a226-5aad-49fc-8b37-c8de723d4ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/dmipy-1.0.5-py3.12.egg/dmipy/utils/utils.py:6: UserWarning: Pass ['name'] as keyword args. From version 2.0.0 passing these as positional arguments will result in an error. \n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/dmipy-1.0.5-py3.12.egg/dmipy/optimizers_fod/csd_cvxpy.py:8: UserWarning: Pass ['name'] as keyword args. From version 2.0.0 passing these as positional arguments will result in an error. \n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/dmipy-1.0.5-py3.12.egg/dmipy/optimizers_fod/csd_tournier.py:6: UserWarning: Pass ['name'] as keyword args. From version 2.0.0 passing these as positional arguments will result in an error. \n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/dmipy-1.0.5-py3.12.egg/dmipy/optimizers_fod/csd_plus.py:10: UserWarning: Pass ['name'] as keyword args. From version 2.0.0 passing these as positional arguments will result in an error. \n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/dmipy-1.0.5-py3.12.egg/dmipy/distributions/distributions.py:19: UserWarning: Pass ['name'] as keyword args. From version 2.0.0 passing these as positional arguments will result in an error. \n"
     ]
    }
   ],
   "source": [
    "# First import the relevant packages and functions\n",
    "from local_optim_fit import reorder_df, forge_axcaliber, fit_dict_axcaliber, fit_params\n",
    "from dmipy.signal_models import gaussian_models, cylinder_models\n",
    "from dmipy.core.acquisition_scheme import acquisition_scheme_from_qvalues,  acquisition_scheme_from_gradient_strengths\n",
    "from dmipy.distributions import distribute_models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from scipy.optimize import curve_fit, minimize\n",
    "import pandas as pd\n",
    "from scipy.stats import gamma\n",
    "from scipy.interpolate import CubicSpline, interp1d\n",
    "from matplotlib.ticker import LinearLocator\n",
    "import copy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba38b97-4a36-48bd-b637-b757d21fd0bb",
   "metadata": {},
   "source": [
    "### Fit the raw data\n",
    "We use two custom functions based on dmipy package and scipy to create our forward model -- the two-compartment model (`forge_axcaliber`), and fit the model to simulated data (`fit_params`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3cd754a-15ae-4ff1-8186-cb5f04aca285",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
      "/Users/zxh380/opt/anaconda3/envs/dmipy/lib/python3.12/site-packages/scipy-1.15.1-py3.12-macosx-10.13-x86_64.egg/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
     ]
    }
   ],
   "source": [
    "# Create lists to store fitted parameters and fitting costs (squared errors)\n",
    "costs = []\n",
    "params = []\n",
    "\n",
    "# Call the custom function to construct the forward model from interpolating simulated dictionary of intraaxonal signals at different radius values.\n",
    "forward_model = forge_axcaliber()\n",
    "\n",
    "# Loop through the relevant MT and radius values and fit the forward model to the total signal \n",
    "for MT in range(20, 151, 10): # Note MT strength defined as the effective T2 it introduces\n",
    "    mt_costs = []\n",
    "    mt_params = []\n",
    "    for r in np.arange(0.3, 5.51, 0.1):\n",
    "        \n",
    "        # read the relevant signal file\n",
    "        simdata = pd.read_csv(\"./MT_results/signal_MT_\"+ str(MT) + \"_sus_0_perm_0.000_rmean_\" + '{0:.2f}'.format(r) + \"_density_0.65.csv\", header=None)\n",
    "        y = simdata[2].to_numpy()/simdata[2][0] # and calculate the DW attenuation\n",
    "        \n",
    "        # Fit the model using the custom function which performs a least square fitting\n",
    "        cost, param = fit_params(forward_model, y)\n",
    "        \n",
    "        # Store the fitting costs and fitted parameters in the correct lists\n",
    "        mt_costs.append(cost)\n",
    "        mt_params.append(param)\n",
    "    costs.append(mt_costs)\n",
    "    params.append(mt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9116c32a-25c8-4f48-bd96-dcadea6cd439",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"precomputed_mt_params.pickle\", 'wb') as file:\n",
    "    pickle.dump(params, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7328686f-fb93-4187-98b5-892b851752dd",
   "metadata": {},
   "source": [
    "To skip fitting parameters from raw data, use the pre-fitted parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e34c74-c0cf-4d99-aba8-3e449525ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"precomputed_mt_params.pickle\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    params = pickle.load( file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892d08b0-f3b6-477e-8ae8-807efa3f2368",
   "metadata": {},
   "source": [
    "A custom function to read the fitted parameters from the dictionary storing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1b8ccce-1f7b-4782-9815-834a389adac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fits(param_dict):\n",
    "\n",
    "    nm_fit = []\n",
    "    for a in param_dict:\n",
    "        nm_fit.append(a[\"Nelder-Mead\"])\n",
    "\n",
    "    nm_fit = np.array(nm_fit)\n",
    "    return nm_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1236967a-caaf-4e06-9f82-99836b89ebfc",
   "metadata": {},
   "source": [
    "### Figure 7a\n",
    "Plot the fitted diameters across the range of underlying diameters at different MT effective T2 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b204767a-6573-4c49-b4ce-4187d3d0c22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List the MT values of interest\n",
    "mts = range(30,151,30)\n",
    "\n",
    "# read the fitted diameter across the whole range of underlying cylinder diameter when there's no MT or permeability (first element of the permeability results)\n",
    "file_path = \"precomputed_perm_params.pickle\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    params_perm = pickle.load(file)\n",
    "nm_fit0 = read_fits(params_perm[0])\n",
    "\n",
    "# For each MT effective T2 values extract the fitted diameters across the whole range of underlying cylinder diameter into a new list\n",
    "for i in range(5):\n",
    "    nm_fit = read_fits( params[i*3+1])\n",
    "    \n",
    "    # Then plot the list of fitted values\n",
    "    plt.plot(np.arange(0.30, 5.51, 0.1)*2,nm_fit[:,2]*2, label=\"Effective $T_2$ = \" + '{0:.1f}'.format(mts[i]) + \"ms\" )\n",
    "\n",
    "# Plot the no MT or permeability case and the true diameter for references\n",
    "plt.plot(np.arange(0.30, 5.51, 0.1)*2, nm_fit0[:,2]*2, label=\"No MT\")\n",
    "plt.plot(np.arange(0.30, 5.51, 0.1)*2,np.arange(0.30, 5.51, 0.1)*2, label=\"True diameter\")\n",
    "plt.xlabel(\"True diameter (m)\")\n",
    "plt.ylabel(\"Estimated diameter (m)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c75817-3961-426a-bcf4-b4683ff5a951",
   "metadata": {},
   "source": [
    "### Figure 7b\n",
    "Plot the fitted volume fractions across the range of underlying diameters at different MT effective T2 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef3d7e6e-04e3-4168-b514-3fa99bad5d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List the MT values of interest\n",
    "mts = range(30,151,30)\n",
    "\n",
    "# read the fitted diameter across the whole range of underlying cylinder diameter when there's no MT or permeability (first element of the permeability results)\n",
    "file_path = \"precomputed_perm_params.pickle\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    params_perm = pickle.load(file)\n",
    "nm_fit0 = read_fits(params_perm[0])\n",
    "\n",
    "# For each MT effective T2 values extract the fitted diameters across the whole range of underlying cylinder diameter into a new list\n",
    "for i in range(5):\n",
    "    nm_fit = read_fits(params[(i+1)*3-2])\n",
    "    \n",
    "    # Then plot the list of fitted values\n",
    "    plt.plot(np.arange(0.30, 5.51, 0.1)*2,nm_fit[:,0], label=\"Effective $T_2$ = \" + '{0:.1f}'.format(mts[i]) + \"ms\" )\n",
    "\n",
    "# Plot the no MT or permeability case and the true diameter for references\n",
    "plt.plot(np.arange(0.30, 5.51, 0.1)*2, nm_fit0[:,0], label=\"No MT\")\n",
    "plt.plot([0.6,11],[0.65,0.65], label=\"True volume fraction\")\n",
    "plt.xlabel(\"True diameter (m)\")\n",
    "plt.ylabel(\"Estimated volume fraction\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c35973-7713-4fbb-b123-5c7be43b0df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1db37-7424-41e8-a467-8c764ff4fbec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
